x <- model.matrix(data$y~., data data[,-1])
x <- model.matrix(data$y~., data =data[,-1])
x <- model.matrix(data$y, data =data[,-1])
data
x <- model.matrix(data$y ~. , data = data[,-1])
x <- model.matrix(data$y, data = data[,-1])
x <- model.matrix(y, data = data[,-1])
x <- model.matrix(y, data = data)
grid <- 10^seq(10, -2, length.out = 100 )
?matrix.model
?model.matrix
x <- model.matrix(data$y , data = data[,-1])
x <- model.matrix(data$y~. , data = data[,-1])
x <- rnorm(100)
e <- rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -2
beta3 = 0.7
y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + e
data <- data.frame("Y" = y, "X1" = x*beta1, "X2" = x^2*beta2, "X3" = beta3*x^3, "e" = e)
reg.fit <- regsubsets(y~poly(x, 10, raw = T) + e, data = data)
summary(reg.fit)
names(reg.fit)
names(summary(reg.fit))
reg.summary <- summary(reg.fit)
reg.summary$rsq
reg.summary$adjr2
par(mfrow = c(2,1))
plot(reg.summary$rss, xlab = "No of variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "No of variables", ylab = "Adj. R^2", type = 'l')
plot(reg.summary$bic, xlab = "No of Variables", ylab = "BIC", type = "l")
mod.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="forward")
mod.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="backward")
fwd.summary = summary(mod.fwd)
bwd.summary = summary(mod.bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
# Plot the statistics
par(mfrow=c(3, 2))
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=1, col="red", lwd=7)
plot(bwd.summary$cp, xlab="Subset Size", ylab="Backward Cp", pch=20, type="l")
points(3, bwd.summary$cp[3], pch=1, col="red", lwd=7)
plot(fwd.summary$bic, xlab="Subset Size", ylab="Forward BIC", pch=20, type="l")
points(3, fwd.summary$bic[3], pch=1, col="red", lwd=7)
plot(bwd.summary$bic, xlab="Subset Size", ylab="Backward BIC", pch=20, type="l")
points(3, bwd.summary$bic[3], pch=1, col="red", lwd=7)
plot(fwd.summary$adjr2, xlab="Subset Size", ylab="Forward Adjusted R2", pch=20, type="l")
points(3, fwd.summary$adjr2[3], pch=1, col="red", lwd=7)
plot(bwd.summary$adjr2, xlab="Subset Size", ylab="Backward Adjusted R2", pch=20, type="l")
points(4, bwd.summary$adjr2[4], pch=1, col="red", lwd=7)
coefficients(mod.fwd, id=3)
coefficients(mod.bwd, id=3)
coefficients(mod.fwd, id=4)
grid <- 10^seq(10, -2, length = 100)
data <- data.frame("Y" = y, "X1" = x, "X2" = x^2, "X3" = x^3, "e" = e)
x <- model.matrix(data$Y~., data = data[,-1])
y <- na.omit(data$Y)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
lasso.mod <- glmnet(x[train,], y[train], lambda = grid, alpha = 1)
summary(lasso.mod)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1 )
summary(cv.out)
plot(cv.out)
best.lam <- (cv.out$lambda.min)
best.lam
lasso.pred <- predict(lasso.mod, s = best.lam, x[test,])
mean((lasso.pred - y.test)^2)
out <- glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = best.lam)
lasso.coef
beta7 <- 7
x <- rnorm(100)
y <- beta0 + beta7*x^7 + e
data <- data.frame("y" = y, "x" <- x)
# Best subset selection
reg.fit <- regsubsets(y ~ poly(x,10, raw = T) , data = data, nvmax = 10)
reg.summ <- summary(reg.fit)
plot(reg.summ$rss, type = 'l', xlab = 'No ov variables', ylab = 'Residual sum of squalres')
plot(reg.summ$bic, type = 'l', xlab = 'No of variables', ylab = 'BIC')
min_rss <- which.min(reg.summ$rss)
min_BiC <- which.max(reg.summ$bic)
reg.back <- regsubsets(y ~ poly(x,10,raw = T), data = data, nvmax = 10, method = 'backward')
reg.back
library(glmnet)
library(leaps)
library(pls)
x <- rnorm(100)
e <- rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -2
beta3 = 0.7
y <- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + e
data <- data.frame("Y" = y, "X1" = x*beta1, "X2" = x^2*beta2, "X3" = beta3*x^3, "e" = e)
reg.fit <- regsubsets(y~poly(x, 10, raw = T) + e, data = data)
summary(reg.fit)
names(reg.fit)
names(summary(reg.fit))
reg.summary <- summary(reg.fit)
reg.summary$rsq
reg.summary$adjr2
par(mfrow = c(2,1))
plot(reg.summary$rss, xlab = "No of variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "No of variables", ylab = "Adj. R^2", type = 'l')
plot(reg.summary$bic, xlab = "No of Variables", ylab = "BIC", type = "l")
mod.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="forward")
mod.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="backward")
fwd.summary = summary(mod.fwd)
bwd.summary = summary(mod.bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
# Plot the statistics
par(mfrow=c(3, 2))
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=1, col="red", lwd=7)
plot(bwd.summary$cp, xlab="Subset Size", ylab="Backward Cp", pch=20, type="l")
points(3, bwd.summary$cp[3], pch=1, col="red", lwd=7)
plot(fwd.summary$bic, xlab="Subset Size", ylab="Forward BIC", pch=20, type="l")
points(3, fwd.summary$bic[3], pch=1, col="red", lwd=7)
plot(bwd.summary$bic, xlab="Subset Size", ylab="Backward BIC", pch=20, type="l")
points(3, bwd.summary$bic[3], pch=1, col="red", lwd=7)
plot(fwd.summary$adjr2, xlab="Subset Size", ylab="Forward Adjusted R2", pch=20, type="l")
points(3, fwd.summary$adjr2[3], pch=1, col="red", lwd=7)
plot(bwd.summary$adjr2, xlab="Subset Size", ylab="Backward Adjusted R2", pch=20, type="l")
points(4, bwd.summary$adjr2[4], pch=1, col="red", lwd=7)
coefficients(mod.fwd, id=3)
coefficients(mod.bwd, id=3)
coefficients(mod.fwd, id=4)
grid <- 10^seq(10, -2, length = 100)
data <- data.frame("Y" = y, "X1" = x, "X2" = x^2, "X3" = x^3, "e" = e)
x <- model.matrix(data$Y~., data = data[,-1])
y <- na.omit(data$Y)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
lasso.mod <- glmnet(x[train,], y[train], lambda = grid, alpha = 1)
summary(lasso.mod)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1 )
summary(cv.out)
plot(cv.out)
best.lam <- (cv.out$lambda.min)
best.lam
lasso.pred <- predict(lasso.mod, s = best.lam, x[test,])
mean((lasso.pred - y.test)^2)
out <- glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = best.lam)
lasso.coef
beta7 <- 7
x <- rnorm(100)
y <- beta0 + beta7*x^7 + e
data <- data.frame("y" = y, "x" <- x)
# Best subset selection
reg.fit <- regsubsets(y ~ poly(x,10, raw = T) , data = data, nvmax = 10)
reg.summ <- summary(reg.fit)
plot(reg.summ$rss, type = 'l', xlab = 'No ov variables', ylab = 'Residual sum of squalres')
plot(reg.summ$bic, type = 'l', xlab = 'No of variables', ylab = 'BIC')
min_rss <- which.min(reg.summ$rss)
min_BiC <- which.max(reg.summ$bic)
reg.back <- regsubsets(y ~ poly(x,10,raw = T), data = data, nvmax = 10, method = 'backward')
reg.back
back.sum <- summary(reg.back)
plot(back.sum$rss, type = 'l', xlab = 'No ov variables', ylab = 'Residual sum of squalres')
plot(back.sum$bic, type = 'l', xlab = 'No of variables', ylab = 'BIC')
plot(back.sum$adjr2, type = 'l', xlab = 'No of variables', ylab = 'adj.R^2')
grid <- 10^seq(10, -2, length.out = 100 )
x <- model.matrix(data$y~. , data = data[,-1])
x <- model.matrix(data$Y~., data = data[,-1])
grid <- 10^seq(10, -2, length = 100)
data <- data.frame("Y" = y, "X1" = x, "X2" = x^2, "X3" = x^3, "e" = e)
x <- model.matrix(data$Y~., data = data[,-1])
data <- data.frame("y" = y, "x" = x)
# Best subset selection
reg.fit <- regsubsets(y ~ poly(x,10, raw = T) , data = data, nvmax = 10)
reg.summ <- summary(reg.fit)
plot(reg.summ$rss, type = 'l', xlab = 'No ov variables', ylab = 'Residual sum of squalres')
plot(reg.summ$bic, type = 'l', xlab = 'No of variables', ylab = 'BIC')
min_rss <- which.min(reg.summ$rss)
min_BiC <- which.max(reg.summ$bic)
reg.back <- regsubsets(y ~ poly(x,10,raw = T), data = data, nvmax = 10, method = 'backward')
reg.back
back.sum <- summary(reg.back)
plot(back.sum$rss, type = 'l', xlab = 'No ov variables', ylab = 'Residual sum of squalres')
plot(back.sum$bic, type = 'l', xlab = 'No of variables', ylab = 'BIC')
plot(back.sum$adjr2, type = 'l', xlab = 'No of variables', ylab = 'adj.R^2')
grid <- 10^seq(10, -2, length.out = 100 )
x <- model.matrix(data$Y~., data = data[,-1])
x <- model.matrix(data$y~., data = data[,-1])
x <- model.matrix(data$y~., data = data[,-1])
x
data <- data.frame("y" = y, "x" = x)
x <- model.matrix(data$y~., data = data[,-1])
x
x <- model.matrix(data$y, data = data[,-1])
data <- data.frame("y" = y, "x" = x, "e" = e)
x <- model.matrix(data$y~., data = data[,-1])
grid <- 10^seq(10, -2, length.out = 100 )
x <- model.matrix(data$y~., data = data[,-1])
x
rm(x)
x
x <- model.matrix(data$y~., data = data[,-1])
x
x[,1]
x[,1:2] <-
x[,1:2]
x[,1:2]
x[,1:3]
x[,1:4]
x[,1:5]
x[,1:3] <- NULL
x[,1:3] <- NA
x
x <- na.omit(x)
x
grid <- 10^seq(10, -2, length.out = 100 )
x <- model.matrix(data$y~., data = data[,-1])
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
?glmnet
lasso.mod <- glmnet(x[train,],y[train], lambda = grid, alpha = 1 )
lasso.mod
summary(lasso.mod)
plot(lasso.mod)
par(mfrow = c(1,1))
plot(lasso.mod)
lasso.mod <- glmnet(x[train,],y[train], lambda = grid, alpha = 0)
summary(lasso.mod)
plot(lasso.mod)
lasso.mod <- glmnet(x[train,],y[train], lambda = grid, alpha = 1)
summary(lasso.mod)
plot(lasso.mod)
?cv.glmnet
cv.out <- cv.glmnet(x[trian,], y[train])
cv.out <- cv.glmnet(x[train,], y[train])
summary(cv.out)
plot(cv.out)
best.lam <- which.min(cv.out$lambda)
best.lam
best.lam <- (cv.out$lambda.min)
best.lam
?predict
lasso.pred <- predict(lasso.mod, s = best.lam, data = x[test,])
lasso.pred <- predict(lasso.mod, s = best.lam, newx = x[test,])
lasso.pred
mean((lasso.pred - y.test)^2)
out <- glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = best.lam)
lasso.coef
lasso.coef
library(ISLR)
attach(College)
College
College['Harvard',]
?College
max[,'Room.Board']
max(College$Room.Board)
College['Oxford',]
College['Cambridge',]
dim(College)
train <- sample(1:nrow(College), 0.7*nrow(College))
test <- (-train)
colnames(College)
College[,1]
count(nrows(College))
nrows(College)
ncols(College)
ncol(College)
x <- College[,2:-15]
?lm
lm.fit <- (Accept ~. , data = College, subset = train)
lm.fit <- (Accept ~., data = College, subset = train)
lm.fit <- (Accept ~., subset = train)
lm.fit <- (Accept~., subset = train)
lm.fit <- (Accept~., data = College[,train])
lm.fit <- (Accept~. data = College[,train])
library(ISLR)
attach(College)
attach(College)
lm.fit <- (Accept~., data=College[,train])
lm.fit <- lm(Accept~., data=College[,train])
lm.fit <- lm(Accept~., data=College[train,])
summary(lm.fit)
plot(lm.fit)
par(mfrow = c(2,2))
plot(lm.fit)
lm.improved <- lm(Accept~Apps + Enroll + Top10perc + Top25perc + Outstate + Expend, data = College[test,])
summary(lm.improved)
plot(lm.imporved)
plot(lm.improved)
lm.improved$residuals
lm.improved$fitted.values
lm.improved$coefficients
lm.pred <- predict(lm.fit, College[test,])
lm.pred
summary(lm.pred)
lm.fit <- lm(Apps~., data=College[train,])
summary(lm.fit)
lm.improved <- lm(Apps~Accept + Enroll + Top10perc + Top25perc + Outstate + Expend + Grad.Rate, data = College[test,])
summary(lm.improved)
lm.fit <- lm(Apps~., data=College[train,])
summary(lm.fit)
lm.improved <- lm(Apps~Accept + Enroll + Top10perc + Top25perc + Outstate + Expend + Grad.Rate, data = College[test,])
summary(lm.improved)
lm.improved$coefficients
plot(lm.improved)
lm.pred <- predict(lm.fit, College[test,])
College.train <- College[train, ]
College.test <- College[test,]
College.test <- College[test, ]
?predict
mean((College.test[,'Apps'] - lm.pred)^2 )
plot(lm.fit)
grid <- 10^seq(10, -2, length.out = 100)
grid <- 10^seq(10, -2, length.out = 1000)
grid
dim(College)
777/18
college.matrix <- model.matrix(College.train[,-1]~.,data = College[train, ])
?regsubsets
colnames(College)
Collete[,!"Apps"]
College[,!"Apps"]
College[,!("Apps")]
College[,"Apps"]
!College[,"Apps"]
Col <- College[,!("Apps")]
sub <- subset(College, select = -('Apps'))
?subset
Col.trian <- College.train[, !(names(College.train) %in% drops)]
drops <- c("Apps")
Col.trian <- College.train[, !(names(College.train) %in% drops)]
college.matrix <- model.matrix(College.trian[,'APPS']~.,data = College.train)
college.matrix <- model.matrix(College.train[,'APPS']~.,data = College.train)
college.matrix <- model.matrix(College.train[,'Apps']~.,data = College.train)
?glmnet
ridge.fit <- glmnet(model.matrix, lambda = grid, alpha = 0)
ridge.fit <- glmnet(model.matrix,College.train$Apps, lambda = grid, alpha = 0)
college.matrix
y.test
ridge.fit <- glmnet(model.matrix, College.train[,'Apps'], lambda = grid, alpha = 0)
ridge.fit <- glmnet(College.train[,'Apps'],model.matrix,lambda = grid, alpha = 0)
head(model.matrix)
head(college.matrix)
college.matrix <- model.matrix(College.train[,'Apps']~.,data = Col.train)
Col.train <- College.train[, !(names(College.train) %in% drops)]
college.matrix <- model.matrix(College.train[,'Apps']~.,data = Col.train)
ridge.fit <- glmnet(College.train[,'Apps'],model.matrix,lambda = grid, alpha = 0)
ridge.fit <- glmnet(y = College.train[,'Apps'], x = model.matrix,lambda = grid, alpha = 0)
dim(model.matrix)
length(model.matrix())
dim(college.matrix)
dim(College.train[,'Apps'])
College.train[,'Apps']
length(College.train[,'Apps'])
dim(college.matrix)
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
summary(ridge.fit)
par(mfrow = c(2,2))
ridge.sum<-summary(ridge.fit)
ridge.sum
ridge.sum$BIC
ridge.sum$bic
par(mfrow = c(1,1))
plot(ridge.fit)
plot(ridge.fit, type = '.')
plot(ridge.fit, type = 'p')
plot(ridge.fit, type = 'p')
plot(ridge.fit)
plot(ridge.fit)
par(mfrow = c(1,1))
plot(ridge.fit)
?cv.glmnet
cv.out <- cv.glmnet(college.matrix,Col.train[,'Apps'], nfolds = 20)
cv.out <- cv.glmnet(college.matrix,Col.train[,'Apps'])
cv.out <- cv.glmnet(college.matrix,College.train[,'Apps'])
summary(cv.out)
best.lam <- cv.out$lambda.min
best.lam
?predict
ridge.pred <- predict(ridge.fit, College.train[,'Apps'], s = best.lam)
ridge.pred <- predict(ridge.fit, s = best.lam, College.train[,'Apps'])
ridge.pred <- predict(ridge.fit, s = best.lam, College.test[,'Apps'])
College.test[,'Apps']
ridge.fit
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
grid <- 10^seq(10, -2, length.out = 1000)
drops <- c("Apps")
college.matrix <- model.matrix(College.train[,'Apps']~.,data = Col.train)
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
ridge.sum<-summary(ridge.fit)
plot(ridge.fit)
college.matrix <- model.matrix(College.train[,'Apps']~.,data = College.train)
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
ridge.sum<-summary(ridge.fit)
par(mfrow = c(1,1))
plot(ridge.fit)
set.seed(1)
cv.out <- cv.glmnet(college.matrix,College.train[,'Apps'])
summary(cv.out)
college.matrix <- model.matrix(College.train[,'Apps']~.,data = College.train)
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
ridge.sum<-summary(ridge.fit)
par(mfrow = c(1,1))
plot(ridge.fit)
ridge.fit
cv.out <- cv.glmnet(college.matrix,College.train[,'Apps'])
summary(cv.out)
best.lam <- cv.out$lambda.min
ridge.pred <- predict(ridge.fit, s = best.lam, College.test[,'Apps'])
best.lam
dim(College.test)
dim(reg.fit)
length(reg.fit)
ridge.pred <- predict(ridge.fit, College.test[,'Apps'])
ridge.pred <- predict(ridge.fit)
ridge.pred <- predict(ridge.fit, newx = College.trest, best.lam)
ridge.pred <- predict(ridge.fit, newx = College.test, best.lam)
ridge.pred <- predict(ridge.fit, newx = college.matrix, best.lam)
college.test.matrix <- model.matrix(College.test[,'Apps']~., data = College.test)
head(college.test.matrix)
ridge.pred <- predict(ridge.fit, newx = college.test.matrix, best.lam)
plot(ridge.pred)
abline(h = 12500, col = 'red')
mean((College.test[,'Apps'] - ridge.pred)^2)
lasso.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 1)
plot(lasso.fit)
mod(lasso.fit)
lasso.fit
lasso.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 1, thresh = 1e-12)
plot(lasso.fit)
lasso.fit
grid <- 10^seq(4, -2, length.out = 1000)
drops <- c("Apps")
college.matrix <- model.matrix(College.train[,'Apps']~.,data = College.train)
college.test.matrix <- model.matrix(College.test[,'Apps']~., data = College.test)
ridge.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 0)
ridge.sum<-summary(ridge.fit)
par(mfrow = c(1,1))
plot(ridge.fit)
set.seed(1)
cv.out <- cv.glmnet(college.matrix,College.train[,'Apps'])
summary(cv.out)
best.lam <- cv.out$lambda.min
ridge.pred <- predict(ridge.fit, newx = college.test.matrix, best.lam)
plot(ridge.pred)
abline(h = 12500, col = 'red')
mean((College.test[,'Apps'] - ridge.pred)^2)
lasso.fit <- glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 1, thresh = 1e-12)
lasso.fit
lasso.fit <- cv.glmnet(college.matrix,College.train[,'Apps'],lambda = grid, alpha = 1, thresh = 1e-12)
plot(lasso.fit)
plot(cv.out)
data <- read.csv("performance_file.csv")
getwd()
# Data visualization
setwd("/Users/nicolasraffray/Desktop/Makers_Course/Final_Project/Net-Positive-Makers/pygym/Performance")
data <- read.csv("performance_file.csv")
# If do not have below library run uncomment below line and run
# install.packages("pracma")
library(pracma)
loss2 <- as.vector(data[[3]])
av <- movavg(loss2, 30)
av100 <- movavg(loss2, 100)
plot(av, type="l",col=rgb(red=0.4, green=0.5, blue=1.0, alpha=0.4))
lines(av100, type="l", col=rgb(red=1.0, green=0.0, blue=0.0, alpha=0.6))
lines(loss2, col = alpha(0.4))
grid(nx = loss2, ny = NULL, col = "lightgray")
grid(nx = NULL, ny = nx, col = "lightgray")
av100 <- movavg(loss2, 1000)
av <- movavg(loss2, 50)
av100 <- movavg(loss2, 1000)
plot(av, type="l",col=rgb(red=0.4, green=0.5, blue=1.0, alpha=0.4))
lines(av100, type="l", col=rgb(red=1.0, green=0.0, blue=0.0, alpha=0.6))
av100 <- movavg(loss2, 100)
RMSloss <- as.vector(RMS[[3]])
av <- movavg(RMSloss, 30)
av100 <- movavg(RMSloss, 100)
plot(av, type="l",col=rgb(red=0.4, green=0.5, blue=1.0, alpha=0.4))
lines(av100, type="l", col=rgb(red=1.0, green=0.0, blue=0.0, alpha=0.6))
lines(loss2, col = alpha(0.4))
grid(nx = loss2, ny = NULL, col = "lightgray")
setwd("/Users/nicolasraffray/Desktop/Makers_Course/Final_Project/Net-Positive-Makers/pygym/Performance")
data <- read.csv("RMSperformance_file.csv")
# If do not have below library run uncomment below line and run
# install.packages("pracma")
library(pracma)
loss2 <- as.vector(data[[3]])
av <- movavg(loss2, 30)
av100 <- movavg(loss2, 100)
plot(av, type="l",col=rgb(red=0.4, green=0.5, blue=1.0, alpha=0.4))
lines(av100, type="l", col=rgb(red=1.0, green=0.0, blue=0.0, alpha=0.6))
lines(loss2, col = alpha(0.4))
View(data)
